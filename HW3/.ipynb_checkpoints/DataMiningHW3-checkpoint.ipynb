{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">EE 380L: Data Mining</p>\n",
    "# <p style=\"text-align: center;\">Assignment 3</p>\n",
    "## <p style=\"text-align: center;\">Total points: 100  </p>\n",
    "## <p style=\"text-align: center;\">Due: October 15th(10/15/2020) submitted via Canvas by 11:59 pm</p>\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UT eID for both students. \n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)*\n",
    "\n",
    "For the descriptive questions, you can write down the solution in paper and embed a picture of it to the notebook or type it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 - Stochastic Gradient Descent (25 pts)\n",
    " 1. (5pts) Using stochastic gradient descent, derive the coefficent updates for all 4 coefficients of the model: \n",
    "$$ y = w_0 + w_1x_1 + w_2x_1x_2 + w_3e^{-x_1} $$ \n",
    "\n",
    "\n",
    " 2. (20pts) Write Python code for an SGD solution to the non-linear model$$ y = w_0 + w_1x_1 + w_2x_1x_2 + w_3e^{-x_1} $$  Try to format similarly to scikit-learn's models. The template of the class is given. The init function of the class takes as input the learning_rate, regularization_constant and number of epochs. The fit method must take as input X,y. The _predict_ method takes an X value (optionally, an array of values). Use your new gradient descent regression to predict the data given in 'samples.csv', for 15 epochs, using learning rates: [0, .0001, .001, .01, 0.1, 1, 10, 100] and regularization (ridge regression) constants: [0,10,100] . Plot MSE and the $w$ parameters as a function of epoch (for 15 epochs) for the best 2 combinations of learning_rate and regularization for SGD. (2pts) Report the MSE at the end of 15 epochs for the two best combinations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "class Regression:\n",
    "    \n",
    "    def __init__(self, learning_rate, regularization, n_epoch):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epoch = n_epoch\n",
    "        self.regularization = regularization\n",
    "        \n",
    "    def sgd(self, gradient):\n",
    "        self.coef # = please fill this to update self.coef using SGD\n",
    "    \n",
    "        \n",
    "    def fit(self, X, y, update_rule='sgd', plot=False):\n",
    "        mse = []\n",
    "        coefs = []\n",
    "        X = self.get_features(X)\n",
    "        for epoch in range(self.n_epoch):\n",
    "            for i in range(X.shape[0]):\n",
    "                # Compute error\n",
    "                   #please fill this\n",
    "                # Compute gradients\n",
    "                    #please fill this\n",
    "               \n",
    "                # Update weights\n",
    "                self.sgd(gradient)\n",
    "\n",
    "            coefs.append(self.coef)\n",
    "            residuals = y - self.linearPredict(X)         \n",
    "            mse.append(np.mean(residuals**2))\n",
    "        self.lowest_mse = mse[-1]\n",
    "        if plot == True:\n",
    "            plt.figure()\n",
    "            plt.plot(range(self.n_epoch),mse)\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('MSE')\n",
    "            plt.figure()\n",
    "            coefs = np.array(coefs)\n",
    "            plt.plot(range(self.n_epoch),coefs[:,0],label='w0')\n",
    "            plt.plot(range(self.n_epoch),coefs[:,1],label='w1')\n",
    "            plt.plot(range(self.n_epoch),coefs[:,2],label='w2')\n",
    "            plt.plot(range(self.n_epoch),coefs[:,3],label='w3')\n",
    "            plt.legend()\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('parameter value')\n",
    "\n",
    "    def get_features(self, X):\n",
    "        '''\n",
    "        this output of this function can be used to compute the gradient in `fit`\n",
    "        '''\n",
    "        x = np.zeros((X.shape[0], 4))\n",
    "        x[:,0] = 1\n",
    "        x[:,1] = X[:,0]\n",
    "        x[:,2] = X[:,0]*X[:,1]\n",
    "        x[:,3] = np.exp(-X[:,0])\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    #def linearPredict(self, X):  \n",
    "        #compute dot product of self.coef and X\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('q1_samples.csv')\n",
    "X = np.array([data['x1'].values, data['x2'].values]).T\n",
    "y = data['y'].values\n",
    "n_epochs = 15\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "regularization = [0, 10, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 - MLP Regressor(15 pts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, you will explore the application of Multi-layer Perceptron (MLP) regression using sklearn package in Python; http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html.\n",
    "\n",
    "The following code will pre-process the data and split the data into training and test sets using train_test_split with random state 30 and test_size = 0.25.\n",
    "\n",
    "The dataset is loaded from q2_data.csv which contains features of engine and the predictor varialble is \"mpg\". The features are:\n",
    "* cylinders \n",
    "* displacement\n",
    "* horsepower\n",
    "* weight\n",
    "* acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(294, 5) (294,) (98, 5) (98,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (train_test_split,KFold)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "df = pd.read_csv('q2_data.csv') \n",
    "df.dropna(inplace=True)\n",
    "df.columns\n",
    "y = df['mpg'].values\n",
    "X = df.drop(['mpg'], axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25, random_state=30)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We also want to use [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). Instead of fitting a model on the original data, we use StandardScaler to center each feature ([Example](http://scikit-learn.org/stable/auto_examples/applications/plot_prediction_latency.html#sphx-glr-auto-examples-applications-plot-prediction-latency-py)). Also remember that when we have training and testing data, we fit preprocessing parameters on training data and apply them to all testing data. You should scale only the features (independent variables), not the target variable y.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   \n",
    "1) (**5pts**) Now, use the parameters for MLP Regressor as below to train a MLPRegressor models on the entire training set and report the RMSE score for both the trainnig and testing sets (again, use StandardScaler) for the different number of hidden units.\n",
    "\n",
    "   (a) *hidden_layer_sizes = (5,)* \n",
    "   \n",
    "   (b) *hidden_layer_sizes = (15,)*\n",
    "   \n",
    "   (c) *hidden_layer_sizes = (50,)*\n",
    "\n",
    "    activation = 'tanh', solver = 'sgd', learning_rate='constant', random_state=42, batch_size= 20,       learning_rate_init = 0.005\n",
    "    \n",
    "   Which of the three models ((a)-(c)) performs the best? Briefly analyze and discuss the results, commenting on the number of hidden units.\n",
    "\n",
    "\n",
    "2) (**5pts**) MLPRegressor has a built-in attribute *loss\\_curve\\_* which returns the loss at each epoch (misleadingly referred to as \"iteration\" in scikit documentation, though they use epoch in the actual code!). For example, if your model is named as *my_model* you can call it as *my\\_model.loss\\_curve\\_* ([example](http://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_training_curves.html#sphx-glr-auto-examples-neural-networks-plot-mlp-training-curves-py)). Plot three curves using the following three conditions (a, b, c) in one figure, where *X-axis* is epoch  number and *Y-axis* is squared root of *loss\\_curve\\_*:\n",
    "\n",
    "   (a) *hidden_layer_sizes = (5,)* \n",
    "   \n",
    "   (b) *hidden_layer_sizes = (15,)*\n",
    "   \n",
    "   (c) *hidden_layer_sizes = (50,)* \n",
    "\n",
    "   \n",
    "3) (**5pts**) Now, use the parameters for MLP Regressor as below to train a MLPRegressor models on the entire training set for different batch size. \n",
    "\n",
    "    activation = 'tanh', solver = 'sgd', learning_rate='constant', random_state=42,learning_rate_init = 0.005 , hidden_layer_sizes = (15,)\n",
    "   \n",
    "   Plot three different figures for the three batch size, where *X-axis* is epoch  number and *Y-axis* is squared root of *loss\\_curve\\_*: \n",
    "\n",
    "   (a) *batch_size = 1* \n",
    "   \n",
    "   (b) *batch_size = 20*  \n",
    "   \n",
    "   (c) *batch_size = 2000* \n",
    "     \n",
    "   **Compare the three plots obtained comment on the gradient updates. What do you understand about the difference between batch_size = 1, 20 and 2000.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Tensor Playground (15 points)\n",
    "Visit https://playground.tensorflow.org for this problem\n",
    "\n",
    "From the far right, select \"Classification\" as the problem type, and select the data set on the top right (alternate colored squares).\n",
    "\n",
    "Use these settings as the DEFAULT settings for all subquestions: test/training ratio = 50%, Noise = 0, Batch Size = 30, learning rate = 0.03, activation = tanh, one hidden layer with 2 neurons, input as $X_1$, $X_2$ and no Regularization.\n",
    "\n",
    "1) Use the DEFAULT setting and run two experiments - one using Tanh as the activation function and one using the linear activation function. Report the train, test losses for both these experiments at the end of 1000 epochs. What qualitative difference do you observe in the decision boundaries obtained? Why? (4 pts)\n",
    "\n",
    "We will now study the effect of certain variations in the network structure or training, keeping all other aspects the same as in the default configuration specified above, and with tanh as the activation for hidden units.\n",
    "\n",
    "2) Effect of number of hidden units: Go back to DEFAULT settings and report the train, test losses at the end of 1000 epochs for 4 and 8 neurons in the hidden layer (2 pairs of results). What do you observe regarding the decision boundary as the number of neurons increases? Why? (4 pts)\n",
    "\n",
    "3) Effect of learning rate and number of epochs: Go back to DEFAULT settings and set the activation to be Relu and have four neurons in the hidden layer. Report the train, test losses at the end of 100 epochs and 1000 epochs for learning rates of 10, 0.1, 0.01 and 0.001 (8 pairs of results). What do you observe in the loss curves? Explain. (4 pts)\n",
    "\n",
    "4) Go back to DEFAULT settings. Play around with any of the other hyperparameters, network architectures and input features (such as $\\sin(X_1), X_1^2$ etc.) and report the best train/test loss you could get (test loss should be at most 0.06). Attach the screenshot from Tensor playground for the same (show your full network, output and parameters). Briefly justify your decisions, and comment on difficulties/tradeoffs, what helps/what doesn't,etc. (3 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4 - Feature Selection (20 pts)\n",
    "\n",
    "In this question, we will explore the importance of feature selection, You can use the below code to load the dataset \"zoo.csv\". This dataset contains a set of features for different animals and the output is class_type \n",
    "corresponding to the features. The details of class_type is present in class.csv for your reference.\n",
    "\n",
    "1. Plot the correlation matrix. The resulting correlation plot should be (n_features * n_features) matrix. List the top 5 most positively correlated features with class_type. (**3pts**)\n",
    "\n",
    "2. List the top 5 most negatively correlated features with class_type. (**3pts**)\n",
    "\n",
    "3. Use https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE to perform feature selection with estimator = LogisticRegression(max_iter=1000) estimator. List the top 5 features selected from RFE.(**2pts**)\n",
    "\n",
    "* Create a train_test split of 80:20(train:test) with random state = 50\n",
    "\n",
    "* We need to use [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). Instead of fitting a model on the original data, we use StandardScaler to center each feature ([Example](http://scikit-learn.org/stable/auto_examples/applications/plot_prediction_latency.html#sphx-glr-auto-examples-applications-plot-prediction-latency-py)). Also remember that when we have training and testing data, we fit preprocessing parameters on training data and apply them to all testing data. You should scale only the features (independent variables), not the target variable y.\n",
    "\n",
    "4. Train a MLP Regressor with train data and report the ${R^2}$ score on the test set for each of the following, \n",
    "\n",
    "   a) Model trained with top 5 most positvely correlated features from part 1\n",
    "   \n",
    "   b) Model trained with top 5 most negatively correlated features from part 2\n",
    "   \n",
    "   c) Model trained with five features selected by RFE from part 3\n",
    "   \n",
    "   d) Finally train a MLP Regressor on the entire training data with the entire orignal features.Use the default values for MLPRegressor as below(**8pts**)\n",
    "\n",
    "    MLPRegressor(max_iter = 1000,random_state=1)\n",
    "\n",
    "5. Explain your results based on your understanding of feature selection and the ${R^2}$ score.  (**4pts**) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"zoo.csv\")\n",
    "df.columns\n",
    "y = df['class_type'].values\n",
    "X = df.drop(['class_type','animal_name'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_df = pd.read_csv(\"class.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 (Data Pre-processing - 25pts)\n",
    "The following dataset contains House Sales data in King County, USA. This dataset contains house sale prices for King County, which includes Seattle. This dataset contains the following columns:\n",
    "\n",
    "* bedrooms: Number of Bedrooms/House\n",
    "* bathrooms: Number of bathrooms/bedrooms\n",
    "* sqft_living: Square footage of the home\n",
    "* sqft_lot: Square footage of the lot\n",
    "* floors: Total floors (levels) in house\n",
    "* waterfront: House which has a view to a waterfront (0/1)\n",
    "* view: Has been viewed (0-4)\n",
    "* condition: How good the condition is Overall (1-5)\n",
    "* grade: Overall grade given to the housing unit, based on King County grading system (1-13)\n",
    "* sqft_above: Square footage of house apart from basement\n",
    "* sqft_basement: Square footage of the basement\n",
    "* yr_built: Built Year\n",
    "* yr_renovated: Year when house was renovated\n",
    "* zipcode: Zip code\n",
    "* price: Selling price of house\n",
    "\n",
    "Here, price is the dependent variable while all other variables are assumed to be independent variables. Here, 10% cells of most columns in the dataset contain NaN value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>5650.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178.0</td>\n",
       "      <td>221900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>7242.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2170.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125.0</td>\n",
       "      <td>538000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>770.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>910.0</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>604000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>8080.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074.0</td>\n",
       "      <td>510000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bedrooms  bathrooms  sqft_living  sqft_lot  floors  waterfront  view  \\\n",
       "0       3.0       1.00       1180.0    5650.0     NaN         0.0   0.0   \n",
       "1       3.0       2.25       2570.0    7242.0     2.0         0.0   0.0   \n",
       "2       2.0       1.00        770.0   10000.0     NaN         0.0   0.0   \n",
       "3       4.0       3.00       1960.0    5000.0     1.0         0.0   0.0   \n",
       "4       3.0       2.00       1680.0    8080.0     1.0         0.0   0.0   \n",
       "\n",
       "   condition  grade  sqft_above  sqft_basement  yr_built  yr_renovated  \\\n",
       "0        3.0    7.0      1180.0            0.0      1955             0   \n",
       "1        3.0    7.0      2170.0          400.0      1951          1991   \n",
       "2        NaN    6.0       770.0            0.0      1933             0   \n",
       "3        5.0    7.0      1050.0          910.0      1965             0   \n",
       "4        3.0    8.0      1680.0            0.0      1987             0   \n",
       "\n",
       "   zipcode     price  \n",
       "0  98178.0  221900.0  \n",
       "1  98125.0  538000.0  \n",
       "2      NaN  180000.0  \n",
       "3      NaN  604000.0  \n",
       "4  98074.0  510000.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "df = pd.read_csv(\"kc_house_data.csv\")\n",
    "age = df['yr_renovated']\n",
    "built = df['yr_built']\n",
    "df.iloc[:, :-1] = df[:-1].mask(np.random.random(df[:-1].shape)<0.1)\n",
    "df['yr_renovated'] = age\n",
    "df['yr_built'] = built\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1 (5 points)\n",
    "Print the number of NaN values in each column. Next, if the value of yr_renovated is equal to 0, set it equal to the corresponding value of yr_built.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2 (6 points)\n",
    "\n",
    "Create a copy of df named df_dm and create a new column in df_dm named binned_yr_built and apply binning to the column yr_built. Use pandas.cut() and modify its paramter list as below:\n",
    "\n",
    "bins=[1900, 1920, 1940, 1960, 1980, 2000, 2020]\n",
    "labels=['1900-1920', '1920-1940', '1940-1960', '1960-1980', '1980-2000', '2000-2020']\n",
    "include_lowest=True\n",
    "\n",
    "Next, perform one-hot encoding using this new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.3 (4 points)\n",
    "\n",
    "Create a copy of df named temp_df. Drop all rows in temp_df that contain any null value and run a linear regression model using a train-test split with test_size=0.3 and random_state=42.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.4 (8 points)\n",
    "\n",
    "Use `df` to create a train/test split with `test_size=0.30` and `random_state=42`. Use the following imputation methods to fill in all NaN values for the obtained training set.\n",
    "- mean (using [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer))\n",
    "- median (using [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer))\n",
    "- most_frequent (using [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer))\n",
    "- k-Nearest Neighbors (using [KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer)) - Run this four times with the value of `n_neighbors` = 2, 5, 10 and 20.\n",
    "\n",
    "For each method, impute the test set with the same imputer and print the test $R^2$ score obtained using a MLPRegressor. Which method gives the best result? What can you say from the result of a) SimpleImputer and b) KNNImputer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.5 (2 points)\n",
    "In Questions 5.3 and 5.4, when you dropped all rows containing any null value, you obtained better $R^2$ score than when you imputed the missing values. What can be some possible reasons behind this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
